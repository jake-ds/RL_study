### day1

- 1. 몬테카를로 예측과 동적 프로그래밍의 차이에 대해 서술해주세요.

몬테카를로 방법은 경험을 기반으로 작동하므로 모델이 필요없는 방법이다. 반면 DP는 모델에 기반한 방법으로 다음 상황에 대한 분포가 필요하다.  


- 2. 활성 정책(On-policy)와 비활성 정책(Off-policy)의 차이에 대해 서술해주세요.

활성정책은 결정을 내리는데 사용되는 정책을 평가하고 향상시키는 반면, 비활성정책은 목적정책과 행동정책을 분리시킨 후 행동정책의 결과를 바탕으로 중요도추출법을 활용한 추정을 통해 목적정책을 학습시킨다. 


- 3. 최초 접촉 MC 방법과 모든 접촉 MC 방법의 차이를 서술해주세요

최초 접촉MC는 상태 S의 최초 경험에서의 보상을 기억하고, 모든 접촉 MC는 상태S의 모든 경험에 대해서 보상의 평균을 기억한다. 


- 4. 중요도 추출법(Importance Sampling)에 대해 설명해주세요

특정 모집단의 분포를 추정하기 어려운 상황에서 다른 모집단의 분포를 활용하여 목적집단의 분포를 추정하는 방법이다. 비활성 정책에서 이 방식을 사용하게 되는데 행동정책의 이득에 중요도추출비율을 곱하여 목적정책의 이득을 추정한다.  


- 5. 최초 접촉 MC 예측(first-visit MC method)의 pseudo-code(p113)을 따라서 적어주세요.

입력: 평가 대상인 정책 pi

초기화 :

모든 s in S에 대해 임의의 값으로 V(s) in R을 초기화

모든 s in S에 대해 값이 채워지지 않은 리스트를 Returns(s) 변수에 대입

(각 에피소드에 대해) 무한루프:

정책 pi를 따르는 하나의 에피소드를 생성: S_0, A_0, R_1, S_1, A_1, R_2, ..., S_T-1, A_T-1, R_T

변수 G에 0을 대입

에피소드의 각 단계에 대해 반복 수행, t=T-1, T-2, ..., 0:

변수 G에 gammaG+R_t+1을 대입

S_t가 S_0, S_1, ..., S_t-1안에 나타나지 않는다면:

리스트 Returns(S_t)에 변수 G를 새 항목으로 추가

리스트 Returns(S_t)에 대한 평균을 V(S_t)에 대입



### day2

- 1. 모든 접촉 MC 예측(every-visit MC method)의 업데이트 식을 암기해서 적어주세요 (식 6.1)

V(S_t) <- V(S_t) +alpha[G_t - V(S_t)]


- 2. TD 오차(TD error)의 공식을 암기해서 적어주세요 (식 6.5)

delta_t = R_t+1+gammaV(S_t+1) - V(S_t)


- 3. TD 학습이 DP 방법에 비해 더 좋은 이유를 설명해주세요. (p151 상단)

환경, 보상, 다음 상태의 확률분포를 필요로하지 않는다. 


- 4. TD 학습이 MC 방법에 비해 더 좋은 이유를 설명해주세요. (p151 상단)

에피소드가 종료될 때까지 기다리지 않아도 된다. 다음 시점으로 넘어가면 상태가치를 추정할 수 있다. 


- 5. SARSA 학습의 Q함수 업데이트 식을 암기해서 적어주세요. (p158)

parameter: 시간간격 alpha in [0, 1], epsilon > 0

모든 s in S+와 a in A(s)에 대해 임의의 값으로 Q(s, a) 초기화. 단, Q(끝,.) = 0

각 에피소드에 대한 루프:

S 초기화

Q로 유도된 정책 사용해 S로 A 선택

에피소드의 각 단계에 대한 루프:

행동 A하고 R, S'관측

Q로 유도된 정책 사용해 S'로 A' 선택

Q(S, A) <- Q(S, A) + alpha[R + gammaQ(S',A')-Q(S,A)]

S<-S';A<-A';

S가 종단이면 종료


- 6. Q 학습의 Q함수 업데이트 식을 암기해서 적어주세요 (p160)

parameter: 시간간격 alpha in [0, 1], epsilon > 0모든 s in S+와 a in A(s)에 대해 임의의 값으로 Q(s, a) 초기화. 단, Q(끝,.) = 0각 에피소드에 대한 루프:S 초기화에피소드의 각 단계에 대한 루프:Q로 유도된 정책 사용해 S로 A 선택행동 A하고 R, S'관측Q(S, A) <- Q(S, A) + alpha[R + gamma max_a Q(S',a)-Q(S,A)]S<-S'S가 종단이면 종료


- 7. 연습문제 6.11. Q 학습이 비활성 정책(off-policy) 방법으로 고려되는 이유는 무엇입니까?

Q함수로부터 최적상태가 선택되지만, 행동가치함수의 목표값은 최적 행동 가치함수 q*에 의한 값을 목표로 한다. 따라서 행동정책과 목표정책에 의한 행동이 다르므로 off-policy 방법으로 고려된다.
