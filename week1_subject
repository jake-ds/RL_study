###day1

- 강화학습이란 무엇인가요?

주어진 상황에서 어떠한 행동을 취할지를 학습하는 것을 의미한다. 행동의 결과는 수치적으로 표현되는 최대의 보상을 가져와야한다. 이 행동의 결과는 이 후의 상황에 영향을 미치므로 이 후의 보상에도 영향을 미치는데 이러한 특징을 지연된 보상이라 한다. 또한 강화학습은 오로지 시행착오를 반복하여 최적의 행동을 찾는다.  

- 탐험과 활용 문제란 무엇인가요?

활용은 과거의 시행착오를 통해 찾은 최적의 정책에 따라 행동하는 것이고, 탐험은 새로운 시도를 통해 최적의 보상을 가져올 가능성이 있는 행동을 찾는 행위이다. 특정시점에서 현재까지 발견한 정책이 최적이라는 것을 확증하기 어렵다. 때문에 활용과 더불어 탐험을 통해 새로운 행동을 취하므로써 기존에 발견하지 못하였던 최적의 행동을 찾을 수 있다. 이 때 탐험과 활용은 트레이드오프 관계에 있다. 

- 정책이란 무엇인가요?

정책이란 학습자(agent)가 특정 상태(state)에서 취해야할 행동의 지침이다. 정책은 다양한 형태(함수, 열람표 등등)가 될 수 있는데, 일반적으로 정책은 특정 상태에서 특정행동을 취할 확률로 정의된다. 

- 보상 신호란 무엇인가요?

보상 신호란 특정 상태에서 학습자가 특정 행위를 취했을 때, 보상으로 주어지는 숫자를 의미한다. 학습자는 시나리오 전 과정에서 보상 신호의 누적 값이 최대가 되는 것을 목표로 학습한다. 

- 가치 함수란 무엇인가요?

가치 함수는 특정 시점에서의 가치를 계산해주는 함수로, 이 때 가치는 특정 시점으로부터 시나리오가 지속될 때 학습자가 얻을 것으로 기대되는 보상의 총합을 의미한다. 가치함수는 정책을 학습데 쓰이는 주요 도구이다. 하지만, 미래의 보상의 총합을 완벽히 계산하는 것은 굉장히 어렵기 때문에 시행착오를 통해 가치를 추정하는 방식으로 학습이 진행된다.

- 모델이란 무엇인가요?

모델은 강화학습이 진행되는 환경으로 특정 상태와 행위를 받고, 다음 상태와 보상을 return하는 것으로 실제의 환경을 모방하는 것이다. 이러한 모델을 활용한 계획을 통해 강화학습 문제를 푸는 방식을 model-based라 하며, 오로지 시행착오를 통해 환경에 대해 학습하는 방법론을 model-free라고 한다.


###day2

- 다중 선택 문제란 무엇인가요?

k개의 서로 다른 옵션이나 행동 중 하나를 반복적으로 선택하여, 선택 후에 주어지는 보상(정상확률분포로 주어짐)의 총량을 최대화하는 문제이다.  이 때 행동은, 행동에 대해 추정된 가치를 기준으로 선택하게 되는데, 최대 가치를 갖는 행동을 탐욕적 행동(지식을 활용하는 것)이라 하고, 최대 가치가 아닌 다른 가치를 가지는 행동을 선택하는 것을 탐험이라 한다. 다중 선택 문제에서 활용과 탐험 사이의 균형을 맞추는 것이 중요하다.

- 신뢰 상한 행동 선택 알고리즘의 공식을 암기해서 적어주세요

At = argmax~a[Qt(a)+c*sqrt(lnt/Nt(a))]

- 맥락적 다중 선택과 강화학습의 차이점에 대해서 설명해주세요

행동이 바로 그 순간의 보상에만 영향을 준다면 맥락적 다중 선택이고, 그 순간의 보상에다가 다음 상황에도 영향을 준다면 강화학습이다. 



###day3

- 1. Bellman Expectation Equation 암기: v(s) (식 3.14)

v(s) = E[Gt|St=s]

- 2. Bellman Optimality Equation 암기: v*(s) (식 3.19)

v*(s)=maxq(s,a) = maxΣp(s',r|s,a)[r+γv*(s')]

- 3. Bellman Optimality Equation 암기: q*(s, a) (식 3.20)

q*(s,a) = Σp(s',r|s,a)[r+γmaxq*(s',a')]



###day4

- 1. Policy Evaluation에서 v(s) 업데이트 식 암기해서 적어주세요: (식 4.4, Bellman Expectation Equation과 동일)

v(s) <- Σπ(a|s)Σp(s',r|s,a)[r+ γv(s')]

- 2. Policy Improvement에서 pi(a|s) 업데이트 식 암기해서 적어주세요: (식 4.9)

π'(s) <- argmaxΣp(s',r|s,a)[r+γv(s')]

- 3. Value Iteration v(s) 업데이트 식 암기해서 적어주세요: (식: 4.10, Bellman Optimality Equation 과 동일)

v(s) <- maxΣp(s',r|s,a)[r+γv(s')]

- 4. 일반화된 정책 반복(Generalized Policy Iteration, GPI)란 무엇인가요?

정책 평가와 정책 향상의 반복주기, 그리고 세부 사항에 관계없이 이 두 과정이 서로 상호작용하게 하는 일반적인 방법이다. 정책 평가와 정책 향상이 상호작용을 하며 반복되면서 결국에는 하나의 공통 해를 찾게 되는데, 이 것이 최적 정책과 최적 가치함수이다.



###day5

- policy iteration

[https://colab.research.google.com/drive/1KyWI4uK-WHC-8emuSwF45lG7tIG1mtwZ?usp=sharing](https://colab.research.google.com/drive/1KyWI4uK-WHC-8emuSwF45lG7tIG1mtwZ?usp=sharing)
